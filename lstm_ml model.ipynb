{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6052,"status":"ok","timestamp":1690266256639,"user":{"displayName":"Music Recommender","userId":"17797157482358442784"},"user_tz":-330},"id":"a6iMe8LyIVqi","outputId":"fceff8e6-577d-4ccf-f701-1a0a78634a8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1690266256640,"user":{"displayName":"Music Recommender","userId":"17797157482358442784"},"user_tz":-330},"id":"gXiZv0z0Z_Fh"},"outputs":[],"source":["from tensorflow.keras.layers import Dense, Concatenate, LSTM, Lambda"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1690266256641,"user":{"displayName":"Music Recommender","userId":"17797157482358442784"},"user_tz":-330},"id":"fftaOuUSaPdM"},"outputs":[],"source":["from tensorflow.keras import Sequential, Model, Input\n","from tensorflow.keras.utils import plot_model"]},{"cell_type":"markdown","metadata":{"id":"PpkVTwUwgBap"},"source":["**Model to predict the type of songs - tap dance, ballet etc**\n","\n","Input - audio and poses of a song\n","Output - type of song (ballet, tap etc) in the form of one hot encoding.\n","\n","One hot encoding - Assume there are 4 types of dances- tap, ballet, hip hop and break. Heres how the outputs would look\n","\n","0001 - tap\n","\n","0010 - ballet\n","\n","0100 - hip hop\n","\n","1000 - break"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1690266256641,"user":{"displayName":"Music Recommender","userId":"17797157482358442784"},"user_tz":-330},"id":"AEzhAgz_nhOl"},"outputs":[],"source":["import numpy as np\n","import librosa\n","import json"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1690266256641,"user":{"displayName":"Music Recommender","userId":"17797157482358442784"},"user_tz":-330},"id":"7b_O0kM0nTZm"},"outputs":[],"source":["#labelling the data1\n","\n","import os\n","\n","folder_path = \"/content/drive/MyDrive/Summer_Internship_Data/Datasets\"  # Replace with the actual path to your \"Datasets\" folder\n","output_array = []\n","\n","# Iterate over the files in the folder\n","for filename in sorted(os.listdir(folder_path)):\n","    if filename.endswith(\"_data.json\"):  # Consider only files ending with \"_data.json\"\n","        name = filename.split(\"_data\")[0]  # Extract the name by removing \"_data\" suffix\n","        dance_form = ''.join([i for i in name if not i.isdigit()])  # Remove numeric part from the name\n","        output_array.append(dance_form)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1690266256642,"user":{"displayName":"Music Recommender","userId":"17797157482358442784"},"user_tz":-330},"id":"teWHpQSetR2K"},"outputs":[],"source":["import os\n","\n","filenames = sorted(os.listdir('/content/drive/MyDrive/Summer_Internship_Data/Datasets'))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1690266256642,"user":{"displayName":"Music Recommender","userId":"17797157482358442784"},"user_tz":-330},"id":"pWiLLh6Ot6Q-","outputId":"36980226-e69f-475b-c655-305a1104755f"},"outputs":[{"data":{"text/plain":["57"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["len(filenames)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690266256642,"user":{"displayName":"Music Recommender","userId":"17797157482358442784"},"user_tz":-330},"id":"unUJGHwyttNO","outputId":"2d7eebc1-93b5-40d8-c789-96178d1a5884"},"outputs":[{"data":{"text/plain":["60"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["filenames = sorted(os.listdir('/content/drive/MyDrive/Summer_Internship_Data/Processed_audios'))\n","len(filenames)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fboRLCwVJcCv"},"outputs":[],"source":["import os\n","\n","\n","pose_path = '/content/drive/MyDrive/Summer_Internship_Data/Datasets'\n","pose_files = os.listdir(pose_path)\n","\n","pose_data = []\n","for pose_data_file in pose_files:\n","  with open(os.path.join(pose_path, pose_data_file), \"r\") as f:\n","      pose_data.append(json.load(f))\n","\n","# Load the audio files and preprocess them (e.g., convert to mel spectrograms)\n","# audio_files = [\"path/to/audio1.wav\", \"path/to/audio2.wav\", ...]\n","audio_path = '/content/drive/MyDrive/Summer_Internship_Data/Processed_audios'\n","audio_files = sorted(os.listdir(audio_path))\n","preprocessed_audio = []\n","for audio_file in audio_files:\n","    audio, sr = librosa.load(os.path.join(audio_path, audio_file), sr=None)  # Load audio file\n","    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr)  # Convert to mel spectrogram\n","    preprocessed_audio.append(spectrogram)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9JxC4UWBgub"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","import numpy as np\n","\n","# Initialize the one-hot encoder\n","onehot_encoder = OneHotEncoder(sparse=False)\n","\n","# Fit and transform the output array with one-hot encoder\n","onehot_encoded = onehot_encoder.fit_transform(np.array(output_array).reshape(-1, 1))\n","\n","# Print the one-hot encoded array\n","print(onehot_encoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyW1pwRowc--"},"outputs":[],"source":["np.array(preprocessed_audio[0]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-IVD0VSv-fA"},"outputs":[],"source":["np.array(preprocessed_audio[1]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENa7KwjeIUrS"},"outputs":[],"source":["import numpy as np\n","\n","# Dataset of shape (26, 128, n)\n","dataset = preprocessed_audio  # Replace ... with the actual variable containing the mel spectrogram data\n","\n","# Step 1: Find the longest length\n","max_length = max([audio_clip.shape[1] for audio_clip in dataset])\n","\n","# Step 2: Insert End of Song token\n","eos_token = np.zeros((128, 1))  # Assuming 128 is the height of the mel spectrogram\n","dataset_with_eos = [np.concatenate((audio_clip, eos_token), axis=1) for audio_clip in dataset]\n","\n","# Step 3: Pad audio clips with 0's\n","padded_dataset = [np.pad(audio_clip, ((0, 0), (0, max_length - audio_clip.shape[1] + 1)), mode='constant', constant_values=0) for audio_clip in dataset_with_eos]\n","\n","# Step 4: Convert the list to a numpy array\n","padded_array = np.array(padded_dataset)\n","\n","print(padded_array.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9dE2qW_xQb1"},"outputs":[],"source":["min_length = float('inf')  # Initialize with a large value\n","\n","for pose_data_file in pose_data:\n","    length = len(pose_data_file)\n","    if length \u003c min_length:\n","        min_length = length\n","\n","print('Minimum length:', min_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-bK5MsO_k-RB"},"outputs":[],"source":["pose_data = [np.array(x)[:min_length, :, :] for x in pose_data]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Hqd36gxcqjT"},"outputs":[],"source":["import numpy as np\n","\n","# Assuming preprocessed_audio is a list of arrays with varying shapes\n","\n","# Step 1: Find the maximum shape\n","max_shape = max([audio.shape for audio in preprocessed_audio])\n","\n","# Step 2: Truncate or pad the arrays to have the same shape\n","padded_audio = []\n","for audio in preprocessed_audio:\n","    # Pad or truncate the array to the maximum shape\n","    padded_audio.append(np.pad(audio, [(0, max_shape[0] - audio.shape[0]), (0, max_shape[1] - audio.shape[1])], mode='constant'))\n","\n","# Step 3: Create the NumPy array with dtype=object\n","preprocessed_audio = np.array(padded_audio, dtype=object)"]},{"cell_type":"markdown","metadata":{"id":"ZduCoWKXwgTh"},"source":["**Your audio files are of different lengths and so are your video files. FIX THIS BEFORE GOING AHEAD**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NleIz3vXoX18"},"outputs":[],"source":["pose_data = np.array(pose_data, dtype=np.float)\n","preprocessed_audio = np.array(preprocessed_audio, dtype=np.float)\n","onehot_encoded = np.array(onehot_encoded, dtype=np.float)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqOd9zIPbQgI"},"outputs":[],"source":["pose_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYtDDDWpaugA"},"outputs":[],"source":["preprocessed_audio.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vV04Zv96axHA"},"outputs":[],"source":["onehot_encoded.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkKVG0ZHblwc"},"outputs":[],"source":["from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Flatten, Conv2D, MaxPooling2D, Dropout\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.models import Model\n","\n","video_input = Input(shape=(911, 33, 3))\n","audio_input = Input(shape=(128, 17183))\n","\n","num_classes = onehot_encoded.shape[1]  # Adjust based on the shape of onehot_encoded\n","\n","x1 = Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(video_input)\n","x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n","x1 = Flatten()(x1)\n","\n","x2 = LSTM(32, kernel_regularizer=regularizers.l2(0.01))(audio_input)\n","\n","x = Concatenate()([x1, x2])\n","x = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","x = Dropout(0.5)(x)\n","output = Dense(num_classes, activation='softmax')(x)\n","\n","model = Model(inputs=[video_input, audio_input], outputs=output)\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model with your data\n","model.fit([pose_data, preprocessed_audio], onehot_encoded, batch_size=8, epochs=100, validation_split=0.2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vm7m01R6mmRY"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Padn8pcBXx_U"},"outputs":[],"source":["import tensorflow as tf\n","def scheduler(epoch, lr):\n","  if epoch \u003c 10:\n","    return lr\n","  else:\n","    return lr * tf.math.exp(-0.1)\n","  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","  callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n","  # Train the model with your data\n","  model.fit([pose_data, preprocessed_audio], onehot_encoded, batch_size=16, epochs=100, callbacks=[callback], validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wijqh6OKaWqe"},"outputs":[],"source":["print(model.evaluate(x = [pose_data, preprocessed_audio], y = onehot_encoded, batch_size = 16))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0p3zGjGckFk"},"outputs":[],"source":["plot_model(model, show_shapes = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D744oS0xhi1l"},"outputs":[],"source":["model.save('dance_classifierv1.hdf5')"]},{"cell_type":"markdown","metadata":{"id":"2SGTFKzu7XiQ"},"source":["**ML Model for dance to music recommender**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bF59rNI77HS"},"outputs":[],"source":["poses = pose_data.copy()\n","np.random.shuffle(poses)\n","\n","audios = preprocessed_audio.copy()\n","np.random.shuffle(audios)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k68wMc1h9iqK"},"outputs":[],"source":["num_positive_samples = len(pose_data)\n","labels_positive = np.ones(num_positive_samples)\n","\n","num_negative_samples = num_positive_samples\n","labels_negative = np.zeros(num_negative_samples)\n","\n","# Combine positive and negative labels\n","labels = np.concatenate((labels_positive, labels_negative), axis=0)\n","\n","# Combine pose inputs and audio inputs for positive and negative samples\n","pose_combined = np.concatenate((pose_data, poses), axis=0)\n","audio_combined = np.concatenate((preprocessed_audio, audios), axis=0)\n","\n","combined_data = list(zip(pose_combined, audio_combined, labels))\n","np.random.shuffle(combined_data)\n","pose_combined, audio_combined, labels_combined = zip(*combined_data)\n","\n","pose_combined = np.array(pose_combined)\n","audio_combined = np.array(audio_combined)\n","labels_combined = np.array(labels_combined)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwOloGzJ9x2R"},"outputs":[],"source":["video_input = Input(shape=(911, 33, 3))\n","audio_input = Input(shape=(128, 17183))\n","\n","x1 = Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(video_input)\n","x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n","x1 = Flatten()(x1)\n","\n","x2 = LSTM(32, kernel_regularizer=regularizers.l2(0.01))(audio_input)\n","\n","x = Concatenate()([x1, x2])\n","x = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","x = Dense(1, activation= 'sigmoid')(x)\n","\n","song_model = Model(inputs = [video_input, audio_input], outputs = x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdfMcqWc_EtA"},"outputs":[],"source":["# def scheduler(epoch, lr):\n","#     if epoch \u003c 10:\n","#         return lr\n","#     else:\n","#         return lr * tf.math.exp(-0.1)\n","\n","# callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wX32EiToFQKM"},"outputs":[],"source":["import tensorflow as tf\n","\n","def scheduler(epoch, lr):\n","    if epoch \u003c 10:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-0.1)\n","\n","class LRSchedulerCallback(tf.keras.callbacks.Callback):\n","    def set_model(self, model):\n","        self.model = model\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        lr = self.model.optimizer.lr\n","        lr.assign(scheduler(epoch, lr))\n","\n","callback = LRSchedulerCallback()\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model with your data\n","model.fit([pose_data, preprocessed_audio], onehot_encoded, batch_size=16, epochs=100, callbacks=[callback], validation_split=0.2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ipt8df54_2cY"},"outputs":[],"source":["song_model.save('song_preds.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HLAkd0G2TUTI"},"outputs":[],"source":["# Load the saved model\n","loaded_model = tf.keras.models.load_model('song_preds.h5')\n","\n","# Assuming pose_data and preprocessed_audio are your test data\n","# Assuming onehot_encoded is your one-hot encoded labels\n","\n","# Reshape the test data to match the input shape of the loaded model\n","pose_data_reshaped = np.reshape(pose_data, (-1, 911, 33, 3))\n","preprocessed_audio_reshaped = np.reshape(preprocessed_audio, (-1, 128, 17183))\n","\n","# Make predictions using the loaded model\n","predictions = loaded_model.predict([pose_data_reshaped, preprocessed_audio_reshaped])\n","\n","# Convert predictions to class labels\n","predicted_classes = np.argmax(predictions, axis=1)\n","\n","# Calculate accuracy\n","accuracy = np.mean(predicted_classes == np.argmax(onehot_encoded, axis=1))\n","\n","# Print the accuracy\n","print('Accuracy:', accuracy)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H67OfE8DgOmU"},"source":["**Model to predict similarity between 2 songs using Siamese Network**\u003cbr/\u003e\n","Input - there are 4 inputs: audio of song 1, poses of song 1, audio of song 2 and video of song 2. \u003cbr/\u003e\n","Output - A score between 0 \u0026 1 of how similar 2 songs are. \u003cbr/\u003e\n","\n","How to use:\n","Predict the score for ever pair of songs. If given a song (audio and video) you are asked to recommend another song, just return the song that has the highest similarity score with your given song.\n","\n","How to train: Watch this youtube video https://www.youtube.com/watch?v=6jfw8MuKwpI\u0026pp=ygUZYW5kcmV3IG5nIHNpYW1lc2UgbmV0d29yaw%3D%3D\n"]},{"cell_type":"markdown","metadata":{"id":"37vbV3yhgyNf"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6y0mMipHhtYK"},"outputs":[],"source":["# import tensorflow as tf\n","# model = tf.keras.models.load('dance_classifier.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v36Ku-bIeOb7"},"outputs":[],"source":["# extract_features = Model(inputs = [video_input, audio_input], outputs = model.layers[-2].output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Of6GvfkueRUf"},"outputs":[],"source":["# from tensorflow.keras import backend as K\n","\n","# pose_input_0 = Input(shape=(5384, 33, 3))\n","# audio_input_0 = Input(shape=(128, 17183))\n","# pose_input_1 = Input(shape=(5384, 33, 3))\n","# audio_input_1 = Input(shape=(128, 17183))\n","\n","# features_1 = extract_features([pose_input_0, audio_input_0])\n","# features_2 = extract_features([pose_input_1, audio_input_1])\n","\n","# def cosine_similarity(vectors):\n","#     x, y = vectors\n","#     x = K.l2_normalize(x, axis=-1)\n","#     y = K.l2_normalize(y, axis=-1)\n","#     return K.sum(x * y, axis=-1, keepdims=True)\n","\n","# similarity = Lambda(cosine_similarity)([features_1, features_2])\n","\n","# # Create the Siamese network\n","# siamese_model = Model(inputs=[pose_input_0, audio_input_0, pose_input_1, audio_input_1], outputs=similarity)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0V1dF8Z1gcts"},"outputs":[],"source":["# plot_model(siamese_model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNmtVf5AhWfb"},"outputs":[],"source":["# siamese_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Mlm0GbchZDW"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","provenance":[{"file_id":"18n4UWYUu1M5p_kNobNgGrhwvTUEhziIT","timestamp":1688617961902}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}